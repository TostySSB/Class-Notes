# 9/4/2019

## Concept Learning
--- 
- Supervized Learning Classification
- Let $C$ be the space of possible concepts (classes) to be learned
- $L$ be a learner
- $H$ be a hypothesis space
- $\epsilon$ be an error bound
- $\delta$ be a confidence measure in an estimate
- size($c$) be the size of the encoding of concept $c$.
- **Given:** Instance space $X$, distribution $D$, set of features $F$, target function $c \ \epsilon \ C$, hypothesis space $H$, and a set of training examples $x \subset X$
- **Find:** Hypothesis $h \ \epsilon \ H$ such that $\forall \ x \epsilon X, \ h(x)=c(x)$
- **Def:** A hpothesis $h$ is said to be consistent with training examples $E\subseteq X \ \iff h(x)=c(x) \ ,\forall \ x \ \epsilon \ E$

## Inductive bias
- Assumptions allowing the model to generalize
  - representation
  - preference => Occam's Razor is the most common preference bias that we use

## Monotone conjunctive concept
- Nothing is negated and the terms are "and'ed" together