# 9/9/2019

## Bias-Variance Tradeoff
---
- generalization
  - overfitting: increase complexity
  - underfitting: decrease complexity
- Assume we are estimating the same parameter $\theta$. Denote the esitmator $d$
    - $MSE(d,\theta)=E[(d-\theta)^2]$
    - $bias_\theta = E[d]-\theta$
    - $var(d) = E[(d-E[d])^2]$
    - $MSE = bias_\theta^2(d) - var(d)$

## Bayesian Decision Theory
---
- This is the basis of project 1
- $A_1, A_1$ 
  - The possible decisions we can make
- $\alpha_1, \alpha_2$
  - The twe decisions we are **gonna** make, or do make
- $Cost \lambda(\alpha_i|A_j)$
  - $R(\alpha_i|e) = \sum_{j=1}^{c}(\alpha_i|A_j)P(A_j|e)$ where R is risk and e is the evidence we have gathered
  - minimize risk
  - $\alpha(e)=arg_{\alpha_i}min \ R(\alpha_i|e)$
  - $P(A_j|e) = P(e|A_j)P(A_j)/P(e)$ = Bayes' Rule